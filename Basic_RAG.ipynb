{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjSVLZe5IQHJOtBQqF+LUN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShirishaReddyV/AI-Sandbox/blob/main/Basic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgPiQHtxxGSB"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb sentence-transformers PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "LocQ68ZTxHDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain.schema import Document\n",
        "\n",
        "def load_pdf_chunks(path, chunk_size=300):\n",
        "    doc = fitz.open(path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "\n",
        "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    docs = [Document(page_content=chunk, metadata={\"chunk_id\": i}) for i, chunk in enumerate(chunks)]\n",
        "    return docs\n",
        "\n",
        "docs = load_pdf_chunks(\"small-business.pdf\")  # update with your file path\n",
        "print(f\"âœ… Loaded {len(docs)} chunks.\")\n"
      ],
      "metadata": {
        "id": "4f-V3MlZxq5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccfb4834"
      },
      "source": [
        "!pip install -U langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(docs, embedding=embedding_model, persist_directory=\"./safety_rag_chroma\")\n",
        "vectordb.persist()\n"
      ],
      "metadata": {
        "id": "MhxfGwKryfXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "771bbe1a"
      },
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(docs, embedding=embedding_model, persist_directory=\"./safety_rag_chroma\")\n",
        "vectordb.persist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_context(query, k=4):\n",
        "    return vectordb.similarity_search(query, k=k)\n",
        "\n",
        "query = \"What are the emergency evacuation steps?\"\n",
        "relevant_docs = get_relevant_context(query)\n",
        "\n",
        "context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "print(\"ðŸ“„ Retrieved Context:\\n\", context)\n"
      ],
      "metadata": {
        "id": "fxl7R-E5yhP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device=\"cpu\")\n",
        "\n",
        "prompt = f\"\"\"Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\"\"\"\n",
        "\n",
        "response = qa_model(prompt, max_new_tokens=150)\n",
        "print(\"ðŸ¤– Answer:\", response[0]['generated_text'].strip())\n"
      ],
      "metadata": {
        "id": "V2XUFMeBzLZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5sHmbVsmzQHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}